{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Breast Cancer Data Analysis\n\n![](https://www.uicc.org/sites/main/files/styles/uicc_news_main_image/public/thumbnails/image/BCAM2016_FA.jpg?itok=zimiEGKS)\n\nIn this tutorial, based on the data we are going to find out if the cancer is benign or malignant. We would use python libraries such as `Numpy`, `Pandas` and `Plotly`. We would use regression techniques to predict the values on our dataset. Source : https://www.kaggle.com/uciml/breast-cancer-wisconsin-data\n\nLet's start off by installing and import the required libraries into our code","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\n%matplotlib inline\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-26T15:21:16.297979Z","iopub.execute_input":"2022-05-26T15:21:16.298331Z","iopub.status.idle":"2022-05-26T15:21:16.374595Z","shell.execute_reply.started":"2022-05-26T15:21:16.298301Z","shell.execute_reply":"2022-05-26T15:21:16.373104Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['font.size'] = 12\nplt.rcParams['figure.figsize'] = (15, 10)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:16.375929Z","iopub.execute_input":"2022-05-26T15:21:16.376282Z","iopub.status.idle":"2022-05-26T15:21:16.381186Z","shell.execute_reply.started":"2022-05-26T15:21:16.376248Z","shell.execute_reply":"2022-05-26T15:21:16.380321Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"raw_df = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')\nraw_df","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:16.382284Z","iopub.execute_input":"2022-05-26T15:21:16.382979Z","iopub.status.idle":"2022-05-26T15:21:16.440853Z","shell.execute_reply.started":"2022-05-26T15:21:16.382935Z","shell.execute_reply":"2022-05-26T15:21:16.440157Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Using the info method on our `dataframe` we are essentially trying to see how many `null` values we have and what is the `datatype` of our columns.","metadata":{}},{"cell_type":"code","source":"raw_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:16.442631Z","iopub.execute_input":"2022-05-26T15:21:16.443108Z","iopub.status.idle":"2022-05-26T15:21:16.464953Z","shell.execute_reply.started":"2022-05-26T15:21:16.443075Z","shell.execute_reply":"2022-05-26T15:21:16.464266Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"We are dropping two columns since they are unnecessary to our training and add no value at all. We stored the values in a new variables and we also drew some graph to explore some interesting facts about our data.","metadata":{}},{"cell_type":"code","source":"df = raw_df.drop(columns=['id', 'Unnamed: 32'])\ndf","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:16.465904Z","iopub.execute_input":"2022-05-26T15:21:16.466226Z","iopub.status.idle":"2022-05-26T15:21:16.503240Z","shell.execute_reply.started":"2022-05-26T15:21:16.466193Z","shell.execute_reply":"2022-05-26T15:21:16.502494Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"fig = px.bar(df, \n            x='radius_mean', \n            y='diagnosis', \n            color='radius_mean',\n            hover_data=['radius_mean'], \n            title='Radius Mean vs Diagnosis')\nfig.update_xaxes(showgrid=False)   #Turning the grid off\nfig.update_yaxes(showgrid=False)   #Turning the grid off\nfig.update_layout({'plot_bgcolor': 'rgba(0, 0, 0, 0)','paper_bgcolor': 'rgba(0, 0, 0, 0)'})  #removing the background color\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:16.506563Z","iopub.execute_input":"2022-05-26T15:21:16.506850Z","iopub.status.idle":"2022-05-26T15:21:17.189930Z","shell.execute_reply.started":"2022-05-26T15:21:16.506824Z","shell.execute_reply":"2022-05-26T15:21:17.189196Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"for template in [\"none\"]:\n    fig = px.bar(df,\n                     x=\"compactness_mean\", \n                     y=\"diagnosis\", \n                     color=\"compactness_mean\",\n                     log_x=True, \n                     template=template, \n                     title=\"Compactness Mean Vs Diagnosis\")\n    fig.update_xaxes(showgrid=False)\n    fig.update_yaxes(showgrid=False)\n    fig.update_layout({'plot_bgcolor': 'rgba(0, 0, 0, 0)','paper_bgcolor': 'rgba(0, 0, 0, 0)'})\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:17.192830Z","iopub.execute_input":"2022-05-26T15:21:17.193103Z","iopub.status.idle":"2022-05-26T15:21:17.244134Z","shell.execute_reply.started":"2022-05-26T15:21:17.193079Z","shell.execute_reply":"2022-05-26T15:21:17.243292Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(df, \n                   x='diagnosis', \n                   color_discrete_sequence=['blue'],\n                   title='Diagnosis Count')\nfig.update_layout(bargap=0.3)\nfig.update_xaxes(showgrid=False)\nfig.update_yaxes(showgrid=False)\nfig.update_layout({'plot_bgcolor': 'rgba(0, 0, 0, 0)','paper_bgcolor': 'rgba(0, 0, 0, 0)'})\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:17.245539Z","iopub.execute_input":"2022-05-26T15:21:17.246117Z","iopub.status.idle":"2022-05-26T15:21:17.325415Z","shell.execute_reply.started":"2022-05-26T15:21:17.246079Z","shell.execute_reply":"2022-05-26T15:21:17.324706Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"From our analysis above, we saw there are 357 Benign Cases and 212 Malignant breast cancer cases. Compactness Mean is more in the Malignant Cases as compared to the Benign Cases.","metadata":{}},{"cell_type":"markdown","source":"## Data Pre-processing\nNow, We are splitting our `df` into `input_cols` as we wanna make sure there is no categorical data since Machine Learning Algorithms cannot work with Categorical data. Fortunately, we don't have the categorical columns in our dataset so we are just using slicing method to make a list of columns and store that into `input-cols`. We used the slicing method to extract the list of `input_cols` and `target_col`.","metadata":{}},{"cell_type":"code","source":"input_cols = df.columns[1:]\ninput_cols","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:17.326588Z","iopub.execute_input":"2022-05-26T15:21:17.327008Z","iopub.status.idle":"2022-05-26T15:21:17.333173Z","shell.execute_reply.started":"2022-05-26T15:21:17.326970Z","shell.execute_reply":"2022-05-26T15:21:17.332357Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"target_col =  df.columns[0]\ntarget_col","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:17.336628Z","iopub.execute_input":"2022-05-26T15:21:17.337251Z","iopub.status.idle":"2022-05-26T15:21:17.343262Z","shell.execute_reply.started":"2022-05-26T15:21:17.337214Z","shell.execute_reply":"2022-05-26T15:21:17.342357Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Since we don't want any data leakage while training our model, we are making a copy of our original dataframe `df` and storing the contents into the new dataframe `inputs_df` and `targets`. `inputs_df` contains all the data that is what we also refer to as independent variable however `targets` contain the dependent variable which means the data in this dataframe is dependent on the independent variables.","metadata":{}},{"cell_type":"code","source":"inputs_df = df[list(input_cols)].copy()\ninputs_df","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:17.344875Z","iopub.execute_input":"2022-05-26T15:21:17.345667Z","iopub.status.idle":"2022-05-26T15:21:17.382658Z","shell.execute_reply.started":"2022-05-26T15:21:17.345631Z","shell.execute_reply":"2022-05-26T15:21:17.381766Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"targets = df[(target_col)]\ntargets","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:17.384111Z","iopub.execute_input":"2022-05-26T15:21:17.384469Z","iopub.status.idle":"2022-05-26T15:21:17.392313Z","shell.execute_reply.started":"2022-05-26T15:21:17.384413Z","shell.execute_reply":"2022-05-26T15:21:17.391176Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Scaling","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(df[input_cols])\ninputs_df[input_cols] = scaler.transform(inputs_df[input_cols])\ninputs_df[input_cols].describe().loc[['min', 'max']]","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:17.394393Z","iopub.execute_input":"2022-05-26T15:21:17.394901Z","iopub.status.idle":"2022-05-26T15:21:17.497082Z","shell.execute_reply.started":"2022-05-26T15:21:17.394857Z","shell.execute_reply":"2022-05-26T15:21:17.496160Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Label Encoding","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ntargets1 = encoder.fit_transform(targets)\ntargets1","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:17.498547Z","iopub.execute_input":"2022-05-26T15:21:17.499076Z","iopub.status.idle":"2022-05-26T15:21:17.509690Z","shell.execute_reply.started":"2022-05-26T15:21:17.499034Z","shell.execute_reply":"2022-05-26T15:21:17.508589Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"We start the process of training our data now that we are done with preprocessing of the data. Lets go ahead and split the data into 2 splits i.e.  training and validation data. Training data will be used to train our model and we will validate the score on the validation data.\n\nWe have taken the test size as 0.25 since we don't want to train our model on the entire dataset and then end up having the model learn nothing when new set of data is thrown at it.","metadata":{}},{"cell_type":"markdown","source":"## Splitting Data","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_inputs, val_inputs, train_targets, val_targets = train_test_split(inputs_df, \n                                                                        targets1, \n                                                                        test_size=0.25, \n                                                                        random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:17.511582Z","iopub.execute_input":"2022-05-26T15:21:17.512265Z","iopub.status.idle":"2022-05-26T15:21:17.520715Z","shell.execute_reply.started":"2022-05-26T15:21:17.512223Z","shell.execute_reply":"2022-05-26T15:21:17.519899Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_inputs.shape, train_targets.shape, val_inputs.shape, val_targets.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:17.522292Z","iopub.execute_input":"2022-05-26T15:21:17.522709Z","iopub.status.idle":"2022-05-26T15:21:17.531021Z","shell.execute_reply.started":"2022-05-26T15:21:17.522667Z","shell.execute_reply":"2022-05-26T15:21:17.529911Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Training our Logistic Regression Model\n\nLogistic regression is a process of modeling the probability of a discrete outcome given an input variable. The most common logistic regression models a binary outcome; something that can take two values such as true/false, yes/no, and so on. Multinomial logistic regression can model scenarios where there are more than two possible discrete outcomes.\n\n![](https://miro.medium.com/max/800/1*UgYbimgPXf6XXxMy2yqRLw.png)\n\nLogistic regression is a useful analysis method for classification problems, where you are trying to determine if a new sample fits best into a category. As aspects of cyber security are classification problems, such as attack detection, logistic regression is a useful analytic technique.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(solver='liblinear')\nmodel.fit(train_inputs, train_targets)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:17.532776Z","iopub.execute_input":"2022-05-26T15:21:17.533234Z","iopub.status.idle":"2022-05-26T15:21:17.549192Z","shell.execute_reply.started":"2022-05-26T15:21:17.533196Z","shell.execute_reply":"2022-05-26T15:21:17.548331Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### Making Predictions","metadata":{}},{"cell_type":"code","source":"train_preds = model.predict(train_inputs)\nval_preds = model.predict(val_inputs)\ntrain_preds, val_preds","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:17.550660Z","iopub.execute_input":"2022-05-26T15:21:17.551079Z","iopub.status.idle":"2022-05-26T15:21:17.568778Z","shell.execute_reply.started":"2022-05-26T15:21:17.551037Z","shell.execute_reply":"2022-05-26T15:21:17.568008Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### Accuracy Score","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nbase_train_score = accuracy_score(train_targets, train_preds)\nprint('The Accuracy score on our training set is {:.2f}%.'.format(base_train_score*100))","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:17.570143Z","iopub.execute_input":"2022-05-26T15:21:17.570720Z","iopub.status.idle":"2022-05-26T15:21:17.579359Z","shell.execute_reply.started":"2022-05-26T15:21:17.570684Z","shell.execute_reply":"2022-05-26T15:21:17.578210Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"base_val_score = accuracy_score(val_targets, val_preds)\nprint('The Accuracy score on our validation set is {:.2f}%.'.format(base_val_score*100))","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:17.581701Z","iopub.execute_input":"2022-05-26T15:21:17.582640Z","iopub.status.idle":"2022-05-26T15:21:17.591227Z","shell.execute_reply.started":"2022-05-26T15:21:17.582590Z","shell.execute_reply":"2022-05-26T15:21:17.590312Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot\ntrain_scores, test_scores = list(), list()\nvalues = [i for i in range(1, 11)]\nfor i in values:\n    # configure the model\n    model = LogisticRegression(solver='liblinear', C=i)\n    # fit model on the training dataset\n    model.fit(train_inputs, train_targets)\n    # evaluate on the train dataset\n    train_preds = model.predict(train_inputs)\n    train_acc = accuracy_score(train_targets, train_preds)\n    train_scores.append(train_acc)\n    # evaluate on the test dataset\n    test_preds = model.predict(val_inputs)\n    test_acc = accuracy_score(val_targets, test_preds)\n    test_scores.append(test_acc)\n    # summarize progress\n    print('%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc))\n# plot of train and test scores vs C Value\npyplot.plot(values, train_scores, '-o', label='Train')\npyplot.plot(values, test_scores, '-o', label='Test')\npyplot.legend()\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:17.593290Z","iopub.execute_input":"2022-05-26T15:21:17.594362Z","iopub.status.idle":"2022-05-26T15:21:17.986078Z","shell.execute_reply.started":"2022-05-26T15:21:17.594324Z","shell.execute_reply":"2022-05-26T15:21:17.985320Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameter Tuning\nBy looking at the chart we could tell that after 4, the training score is getting better but the validation/test score has come to a stand still which is why probably the best value for C is 4. Let's try to tune the parameters now","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nC_range = np.arange(1,11,1)\npenalty_range= ['l2','l1']\nmax_iter_range = np.arange(1,110,10)\nparam_grid = dict(C=C_range, penalty=penalty_range, max_iter= max_iter_range)\nmodel = LogisticRegression(solver='liblinear',)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:17.987427Z","iopub.execute_input":"2022-05-26T15:21:17.987807Z","iopub.status.idle":"2022-05-26T15:21:17.993675Z","shell.execute_reply.started":"2022-05-26T15:21:17.987770Z","shell.execute_reply":"2022-05-26T15:21:17.992935Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"grid.fit(train_inputs, train_targets)\nprint(\"The best parameters are %s with a score of %0.2f\"\n      % (grid.best_params_, grid.best_score_))","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:17.995312Z","iopub.execute_input":"2022-05-26T15:21:17.995990Z","iopub.status.idle":"2022-05-26T15:21:30.161135Z","shell.execute_reply.started":"2022-05-26T15:21:17.995954Z","shell.execute_reply":"2022-05-26T15:21:30.160289Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"model = LogisticRegression(solver='liblinear', C= 4, max_iter= 11, penalty= 'l1')\nmodel.fit(train_inputs, train_targets)\ntrain_preds = model.predict(train_inputs)\nval_preds = model.predict(val_inputs)\ntrain_score = accuracy_score(train_targets, train_preds)\nval_score = accuracy_score(val_targets, val_preds)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:30.165358Z","iopub.execute_input":"2022-05-26T15:21:30.167840Z","iopub.status.idle":"2022-05-26T15:21:30.207218Z","shell.execute_reply.started":"2022-05-26T15:21:30.167808Z","shell.execute_reply":"2022-05-26T15:21:30.206390Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"model_scores = pd.DataFrame({\n    'Base Train Score': base_train_score.flatten()*100,\n    'Base Val Score': base_val_score.flatten()*100,\n    'New Train Score': train_score.flatten()*100,\n    'New Val Score': val_score.flatten()*100\n                })\nmodel_scores","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:30.211430Z","iopub.execute_input":"2022-05-26T15:21:30.213672Z","iopub.status.idle":"2022-05-26T15:21:30.234570Z","shell.execute_reply.started":"2022-05-26T15:21:30.213633Z","shell.execute_reply":"2022-05-26T15:21:30.233789Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## Confusion Matrix\n\nThe model achieves an accuracy of 96.71% on the training set. We can visualize the breakdown of correctly and incorrectly classified inputs using a confusion matrix.\n\n<img src=\"https://i.imgur.com/UM28BCN.png\" width=\"480\">","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusionmatrix = confusion_matrix(train_targets, train_preds, normalize='true')\nplt.figure(figsize=(14,7)) \nsns.heatmap(confusionmatrix*100, annot=True,annot_kws={\"size\": 18}, cmap=\"Blues\")\nplt.xlabel('Prediction',fontsize=16)\nplt.ylabel('Target',fontsize=16)\nplt.title('Confusion Matrix', fontsize=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:30.238143Z","iopub.execute_input":"2022-05-26T15:21:30.238809Z","iopub.status.idle":"2022-05-26T15:21:30.563160Z","shell.execute_reply.started":"2022-05-26T15:21:30.238766Z","shell.execute_reply":"2022-05-26T15:21:30.562359Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"From the matrix above, we come to a conclusion that it did a good job at predicting True Positive i.e first quadrant however it could do a better job at predicting the true negatives.","metadata":{}},{"cell_type":"markdown","source":"### Feature Importance\nLet's look at the weights assigned to different columns, to figure out which columns in the dataset are the most important.","metadata":{}},{"cell_type":"code","source":"weights = model.coef_\nweights = weights.reshape(30)\nweights_df = pd.DataFrame({\n    'Feature': input_cols,\n    'Weight': weights\n                }).sort_values('Weight', ascending=False)\nweights_df","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:30.577078Z","iopub.execute_input":"2022-05-26T15:21:30.579250Z","iopub.status.idle":"2022-05-26T15:21:30.615166Z","shell.execute_reply.started":"2022-05-26T15:21:30.579210Z","shell.execute_reply":"2022-05-26T15:21:30.614465Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"fig = px.scatter(weights_df, x=\"Feature\", y=\"Weight\", title='Feature Importance Chart',color=\"Weight\")\nfig.update_xaxes(showgrid=False)\nfig.update_yaxes(showgrid=False)\nfig.update_layout({'plot_bgcolor': 'rgba(0, 0, 0, 0)','paper_bgcolor': 'rgba(0, 0, 0, 0)'})\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:30.616054Z","iopub.execute_input":"2022-05-26T15:21:30.616355Z","iopub.status.idle":"2022-05-26T15:21:30.739543Z","shell.execute_reply.started":"2022-05-26T15:21:30.616324Z","shell.execute_reply":"2022-05-26T15:21:30.738772Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"From the charts above, we see that `concave points_mean` is the most important feature in determining if its a malignant or benign case of breast cancer with the highest weight of 2.388964","metadata":{}},{"cell_type":"markdown","source":"## Decision Tree Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(random_state=42)\nmodel.fit(train_inputs, train_targets)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:30.740980Z","iopub.execute_input":"2022-05-26T15:21:30.741553Z","iopub.status.idle":"2022-05-26T15:21:30.761197Z","shell.execute_reply.started":"2022-05-26T15:21:30.741516Z","shell.execute_reply":"2022-05-26T15:21:30.760562Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix\ntrain_preds = model.predict(train_inputs)\ntrain_score = model.score(train_inputs, train_targets)*100\ntrain_score","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:30.764659Z","iopub.execute_input":"2022-05-26T15:21:30.766560Z","iopub.status.idle":"2022-05-26T15:21:30.782435Z","shell.execute_reply.started":"2022-05-26T15:21:30.766523Z","shell.execute_reply":"2022-05-26T15:21:30.781797Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"val_preds = model.predict(val_inputs)\nval_score = model.score(val_inputs, val_targets)*100\nval_score","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:30.785990Z","iopub.execute_input":"2022-05-26T15:21:30.787981Z","iopub.status.idle":"2022-05-26T15:21:30.802609Z","shell.execute_reply.started":"2022-05-26T15:21:30.787940Z","shell.execute_reply":"2022-05-26T15:21:30.801842Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"model.tree_.max_depth","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:30.806438Z","iopub.execute_input":"2022-05-26T15:21:30.808398Z","iopub.status.idle":"2022-05-26T15:21:30.817642Z","shell.execute_reply.started":"2022-05-26T15:21:30.808362Z","shell.execute_reply":"2022-05-26T15:21:30.816748Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import plot_tree, export_text\nplt.figure(figsize=(80,20))\nplot_tree(model, feature_names=train_inputs.columns, max_depth=2, filled=True);","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:30.822159Z","iopub.execute_input":"2022-05-26T15:21:30.824232Z","iopub.status.idle":"2022-05-26T15:21:32.301232Z","shell.execute_reply.started":"2022-05-26T15:21:30.824198Z","shell.execute_reply":"2022-05-26T15:21:32.300496Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"Diagnosis = {\n    'M' : '1',\n    'B' : '0'\n            }\nDiagnosis","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:32.302216Z","iopub.execute_input":"2022-05-26T15:21:32.302549Z","iopub.status.idle":"2022-05-26T15:21:32.309890Z","shell.execute_reply.started":"2022-05-26T15:21:32.302519Z","shell.execute_reply":"2022-05-26T15:21:32.309042Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"confusionmatrix = confusion_matrix(train_targets, train_preds, normalize='true')\nplt.figure(figsize=(14,7)) \nsns.heatmap(confusionmatrix, annot=True,annot_kws={\"size\": 18}, cmap=\"Greens\")\nplt.xlabel('Prediction',fontsize=16)\nplt.ylabel('Target',fontsize=16)\nplt.title('Confusion Matrix', fontsize=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:32.311482Z","iopub.execute_input":"2022-05-26T15:21:32.312210Z","iopub.status.idle":"2022-05-26T15:21:32.587557Z","shell.execute_reply.started":"2022-05-26T15:21:32.312173Z","shell.execute_reply":"2022-05-26T15:21:32.586670Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"max_depth_range = np.arange(1,8,1)\nmax_features_range= np.arange(1,31,1)\nmax_leaf_nodes_range = np.arange(2,100,10)\nparam_grid = dict(max_depth=max_depth_range, max_features=max_features_range, max_leaf_nodes=max_leaf_nodes_range)\nmodel = DecisionTreeClassifier(random_state=42)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\ngrid.fit(train_inputs, train_targets)\nprint(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:21:32.592161Z","iopub.execute_input":"2022-05-26T15:21:32.594610Z","iopub.status.idle":"2022-05-26T15:22:34.182224Z","shell.execute_reply.started":"2022-05-26T15:21:32.594567Z","shell.execute_reply":"2022-05-26T15:22:34.181374Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"model = DecisionTreeClassifier(max_depth = 5, max_features= 3, max_leaf_nodes= 11, random_state=42)\nmodel.fit(train_inputs, train_targets)\ntrain_preds = model.predict(train_inputs)\nval_preds = model.predict(val_inputs)\ntrain_score = accuracy_score(train_targets, train_preds)\nval_score = accuracy_score(val_targets, val_preds)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:22:34.183654Z","iopub.execute_input":"2022-05-26T15:22:34.184182Z","iopub.status.idle":"2022-05-26T15:22:34.197127Z","shell.execute_reply.started":"2022-05-26T15:22:34.184144Z","shell.execute_reply":"2022-05-26T15:22:34.196332Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"model_scores = pd.DataFrame({\n    'Base Train Score': base_train_score.flatten()*100,\n    'Base Val Score': base_val_score.flatten()*100,\n    'New Train Score': train_score.flatten()*100,\n    'New Val Score': val_score.flatten()*100\n                })\nmodel_scores","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:22:34.198673Z","iopub.execute_input":"2022-05-26T15:22:34.199156Z","iopub.status.idle":"2022-05-26T15:22:34.212624Z","shell.execute_reply.started":"2022-05-26T15:22:34.199106Z","shell.execute_reply":"2022-05-26T15:22:34.211755Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest Classifier\n\nWhile tuning the hyperparameters of a single decision tree may lead to some improvements, a much more effective strategy is to combine the results of several decision trees trained with slightly different parameters. This is called a random forest model.\n\nThe key idea here is that each decision tree in the forest will make different kinds of errors, and upon averaging, many of their errors will cancel out. This idea is also commonly known as the \"wisdom of the crowd\".\n\nA random forest works by averaging/combining the results of several decision trees:\n\n![](https://1.bp.blogspot.com/-Ax59WK4DE8w/YK6o9bt_9jI/AAAAAAAAEQA/9KbBf9cdL6kOFkJnU39aUn4m8ydThPenwCLcBGAsYHQ/s0/Random%2BForest%2B03.gif)\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_jobs=-1, random_state=42)\nmodel.fit(train_inputs,train_targets)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:22:34.213957Z","iopub.execute_input":"2022-05-26T15:22:34.214285Z","iopub.status.idle":"2022-05-26T15:22:34.470935Z","shell.execute_reply.started":"2022-05-26T15:22:34.214252Z","shell.execute_reply":"2022-05-26T15:22:34.470200Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"model_train_score = model.score(train_inputs, train_targets)*100\nmodel_val_score = model.score(val_inputs, val_targets)*100\nprint('Random Forest Training Score - {:.2f}%'.format(model_train_score))\nprint('Random Forest Validation Score - {:.2f}%'.format(model_val_score))","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:22:34.472249Z","iopub.execute_input":"2022-05-26T15:22:34.472618Z","iopub.status.idle":"2022-05-26T15:22:34.686522Z","shell.execute_reply.started":"2022-05-26T15:22:34.472582Z","shell.execute_reply":"2022-05-26T15:22:34.685650Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"forest_weights = model.feature_importances_\nweights_df = pd.DataFrame({\n    'Feature': input_cols,\n    'Weight': forest_weights\n                }).sort_values('Weight', ascending=False)\nplt.figure(figsize=(20,10))\nplt.xticks(rotation=45)\nplt.title('Feature Importance')\nsns.barplot(data=weights_df.head(10), x='Feature', y='Weight');","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:22:34.688478Z","iopub.execute_input":"2022-05-26T15:22:34.689472Z","iopub.status.idle":"2022-05-26T15:22:35.069320Z","shell.execute_reply.started":"2022-05-26T15:22:34.689414Z","shell.execute_reply":"2022-05-26T15:22:35.066721Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"### HyperParameter Tuning","metadata":{}},{"cell_type":"code","source":"max_depth_range = np.arange(1,8,1)\nmax_features_range= np.arange(1,31,1)\nmax_leaf_nodes_range = np.arange(2,100,10)\nfrom sklearn.model_selection import RandomizedSearchCV\ndistributions = dict(max_depth=max_depth_range, max_features=max_features_range, max_leaf_nodes=max_leaf_nodes_range)\nmodel = RandomForestClassifier(n_jobs=-1, random_state=42)\nclf = RandomizedSearchCV(model, distributions, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:22:35.070914Z","iopub.execute_input":"2022-05-26T15:22:35.071348Z","iopub.status.idle":"2022-05-26T15:22:35.080808Z","shell.execute_reply.started":"2022-05-26T15:22:35.071305Z","shell.execute_reply":"2022-05-26T15:22:35.079891Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"clf.fit(train_inputs, train_targets)\nprint(\"The best parameters are %s with a score of %0.2f\" % (clf.best_params_, clf.best_score_))","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:22:35.083284Z","iopub.execute_input":"2022-05-26T15:22:35.084204Z","iopub.status.idle":"2022-05-26T15:22:57.060276Z","shell.execute_reply.started":"2022-05-26T15:22:35.084163Z","shell.execute_reply":"2022-05-26T15:22:57.059187Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"model = RandomForestClassifier(max_depth= 6, \n                               max_leaf_nodes= 82,\n                               max_features=14,\n                               n_jobs=-1,\n                               random_state=42)\nmodel.fit(train_inputs,train_targets)\nmodel_train_score = model.score(train_inputs, train_targets)*100\nmodel_val_score = model.score(val_inputs, val_targets)*100\nprint('Random Forest Training Score - {:.2f}%'.format(model_train_score))\nprint('Random Forest Validation Score - {:.2f}%'.format(model_val_score))","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:22:57.062041Z","iopub.execute_input":"2022-05-26T15:22:57.062798Z","iopub.status.idle":"2022-05-26T15:22:57.634134Z","shell.execute_reply.started":"2022-05-26T15:22:57.062742Z","shell.execute_reply":"2022-05-26T15:22:57.633195Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"model_scores = pd.DataFrame({\n    'Base Train Score': base_train_score.flatten()*100,\n    'Base Val Score': base_val_score.flatten()*100,\n    'New Train Score': model_train_score.flatten(),\n    'New Val Score': model_val_score.flatten()\n                })\nmodel_scores","metadata":{"execution":{"iopub.status.busy":"2022-05-26T15:22:57.635903Z","iopub.execute_input":"2022-05-26T15:22:57.636356Z","iopub.status.idle":"2022-05-26T15:22:57.650517Z","shell.execute_reply.started":"2022-05-26T15:22:57.636310Z","shell.execute_reply":"2022-05-26T15:22:57.649496Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"## Principal Component Analysis (PCA)\n\nPrincipal Component Analysis is a way to reduce the number of variables while maintaining the majority of the important information. It transforms a number of variables that may be correlated into a smaller number of uncorrelated variables, known as principal components.\n\nThe main objective of PCA is to simplify your model features into fewer components to help visualize patterns in your data and to help your model run faster. Using PCA also reduces the chance of overfitting your model by eliminating features with high correlation.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import scale\nfrom sklearn import decomposition #PCA\nX = scale(inputs_df)\npca = decomposition.PCA(n_components=5)\npca.fit(X)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:15:31.725666Z","iopub.execute_input":"2022-05-26T16:15:31.726376Z","iopub.status.idle":"2022-05-26T16:15:31.741376Z","shell.execute_reply.started":"2022-05-26T16:15:31.726335Z","shell.execute_reply":"2022-05-26T16:15:31.740358Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"scores = pca.transform(X)\nscores_df = pd.DataFrame(scores, columns=['PC1', 'PC2','PC3', 'PC4', 'PC5'])\ntarget = pd.Series(targets1, name='target')\nresult_df = pd.concat([scores_df, target], axis=1)\nresult_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:04:41.008049Z","iopub.execute_input":"2022-05-26T16:04:41.008396Z","iopub.status.idle":"2022-05-26T16:04:41.025332Z","shell.execute_reply.started":"2022-05-26T16:04:41.008366Z","shell.execute_reply":"2022-05-26T16:04:41.024220Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (12,10))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('First Principal Component ', fontsize = 15)\nax.set_ylabel('Second Principal Component ', fontsize = 15)\n\nax.set_title('Principal Component Analysis (5PCs) for Cancer Dataset', fontsize = 20)\n\ntargets = [0, 1]\ncolors = ['r', 'g']\nfor target, color in zip(targets, colors):\n    indicesToKeep = targets1 == target\n    ax.scatter(result_df.loc[indicesToKeep, 'PC1'], \n               result_df.loc[indicesToKeep, 'PC2'],\n               c = color, \n               s = 50)\nax.legend(targets)\nax.grid()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:13:33.187138Z","iopub.execute_input":"2022-05-26T16:13:33.187521Z","iopub.status.idle":"2022-05-26T16:13:33.451769Z","shell.execute_reply.started":"2022-05-26T16:13:33.187487Z","shell.execute_reply":"2022-05-26T16:13:33.450999Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"markdown","source":"**Explained Variance Ratio**\n\nThe explained variance ratio is the percentage of variance that is attributed by each of the selected components. Ideally, you would choose the number of components to include in your model by adding the explained variance ratio of each component until you reach a total of around 0.8 or 80% to avoid overfitting.","metadata":{}},{"cell_type":"code","source":"print('Variance of each component:', pca.explained_variance_ratio_)\nprint('\\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2))","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:15:37.493859Z","iopub.execute_input":"2022-05-26T16:15:37.494249Z","iopub.status.idle":"2022-05-26T16:15:37.500951Z","shell.execute_reply.started":"2022-05-26T16:15:37.494216Z","shell.execute_reply":"2022-05-26T16:15:37.499851Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"markdown","source":"We can see that our first two principal components explain the majority of the variance in this dataset (84.73%)! This is an indication of the total information represented compared to the original data.","metadata":{}},{"cell_type":"markdown","source":"## Support Vector Machine\n\nThe objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N â€” the number of features) that distinctly classifies the data points.\n\n![](https://miro.medium.com/max/1400/0*ecA4Ls8kBYSM5nza.jpg)\n\nSupport vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These are the points that help us build our SVM.\n\n\nIn the SVM algorithm, we are looking to maximize the margin between the data points and the hyperplane.","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nmodel = SVC(kernel='linear')\nmodel.fit(train_inputs, train_targets)\nval_preds = model.predict(val_inputs)\nSVMAccuracy = accuracy_score(val_targets,val_preds)*100\nprint('SVM Accuracy Score - {:.2f}%'.format(SVMAccuracy))","metadata":{"execution":{"iopub.status.busy":"2022-05-26T16:54:11.803644Z","iopub.execute_input":"2022-05-26T16:54:11.804290Z","iopub.status.idle":"2022-05-26T16:54:11.821008Z","shell.execute_reply.started":"2022-05-26T16:54:11.804253Z","shell.execute_reply":"2022-05-26T16:54:11.820217Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"markdown","source":"We can clearly see that SVM has given us the best accuracy score i.e 98.60 without much hyperparameter tuning. \n\n\nSUMMARY OF THE NOTEBOOK:-\n1. 357 Benign Cases and 212 Malignant breast cancer cases. Compactness Mean is more in the Malignant Cases as compared to the Benign Cases.\n2. Depending upon the data and the computational power, one should use GridSearch or RandomizedSearch for hyperparameter tuning\n3. PCA is a great way to shift from high dimensionality to low dimensionality. One must choose the number of components (while performing PCA) to include in your model by adding the explained variance ratio of each component until you reach a total of around 0.8 or 80% to avoid overfitting.\n4. Relying on complex algorithms always should not be the way out. Sometimes, even a simpler algorithms can work wonders.","metadata":{}},{"cell_type":"markdown","source":"**Further reading **\n- https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47\n- https://scikit-learn.org/stable/modules/svm.html\n- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n\nIf you like the notebook, please be sure to leave an upvote on your way out. THANKS FOR YOUR TIME!!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}